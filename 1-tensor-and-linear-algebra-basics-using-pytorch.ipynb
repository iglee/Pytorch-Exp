{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Different types of tensors using pytorch\n",
    "\n",
    "1. empty tensor - uninitialized.  the tensor dimensions can be specified, but it's empty.\n",
    "\n",
    "`torch.empty(DIMENSIONS)`\n",
    "2. randomly initialized tensor\n",
    "\n",
    "`torch.rand(DIMENSIONS)`\n",
    "3. tensor initialized with zeros\n",
    "\n",
    "`torch.zeros(DIMENSIONS)`\n",
    "4. tensor initialized with ones\n",
    "\n",
    "`torch.ones(DIMENSIONS)`\n",
    "5. randomly initialized with normal distribution random numbers $\\mathcal{N} (\\mu = 0, \\sigma^2 = 1)$\n",
    "\n",
    "`torch.randn_like(DIMENSIONS)` $\\rightarrow$ we can initialize using `_like()` functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try random initialization with normal distribution in 5.  Given a tensor, `randn_like` will initialize that function with normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_randn = torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure, we can try to plot the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x, sorted_indices = x_randn.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sorted_x, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cool.  I'm better convinced that this is normally random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Operations\n",
    "\n",
    "1. [Addition](#addition)\n",
    "2. [in-place operations](#in-place-operations)\n",
    "3. [mathematical operations](#mathematical-operations)\n",
    "4. [broadcasting](#broadcasting)\n",
    "5. [matrix multiplication](#matrix-multiplication)\n",
    "    - dot product\n",
    "    - matrix product\n",
    "6. changing dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='addition'></a>\n",
    "### 2.1 Addition \n",
    "\n",
    "First, we define a couple tensors for our purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8049, -0.0204]],\n",
       "\n",
       "         [[ 0.2558, -0.8377]],\n",
       "\n",
       "         [[-0.1351,  0.5858]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3349,  0.6151]],\n",
       "\n",
       "         [[ 1.2394,  1.4540]],\n",
       "\n",
       "         [[ 0.3045, -0.9893]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at this, let's break down the dimensions and count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([\n",
    "        [    \n",
    "             [   [ 1.1677,  0.2297]   ],\n",
    "\n",
    "             [   [-0.5983,  1.2568]   ],\n",
    "\n",
    "             [   [-1.2132,  0.0813]   ]\n",
    "        ],\n",
    "\n",
    "\n",
    "        [    \n",
    "             [   [-0.4010,  0.4659]   ],\n",
    "\n",
    "             [   [ 1.2019,  0.5684]   ],\n",
    "\n",
    "             [   [ 0.5998,  0.2074]   ]\n",
    "        ]\n",
    "       ])\n",
    "```\n",
    "Spacing out to make clear how dimensions are represented in pytorch.  Note that the $3^{rd}$ dimension is not represented, since it's 1.\n",
    "\n",
    "Now we define couple more vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(2,3,1,2)\n",
    "z = torch.randn(2,3,2  )\n",
    "a = torch.randn(  3,1,2)\n",
    "b = torch.randn(2,3,1  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use various representation to add two tensors: `+`, `torch.add(tensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.1516, -0.3126]],\n",
       "\n",
       "         [[-0.8538,  0.1927]],\n",
       "\n",
       "         [[ 0.3149,  0.9242]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3320,  2.0967]],\n",
       "\n",
       "         [[ 0.3307,  4.0127]],\n",
       "\n",
       "         [[-0.4137, -1.6834]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.1516, -0.3126]],\n",
       "\n",
       "         [[-0.8538,  0.1927]],\n",
       "\n",
       "         [[ 0.3149,  0.9242]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3320,  2.0967]],\n",
       "\n",
       "         [[ 0.3307,  4.0127]],\n",
       "\n",
       "         [[-0.4137, -1.6834]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.1516, -0.3126]],\n",
       "\n",
       "         [[-0.8538,  0.1927]],\n",
       "\n",
       "         [[ 0.3149,  0.9242]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3320,  2.0967]],\n",
       "\n",
       "         [[ 0.3307,  4.0127]],\n",
       "\n",
       "         [[-0.4137, -1.6834]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= in-place-operations></a>\n",
    "### 2.2 In-place operations\n",
    "\n",
    "Sometimes we want to operate on a tensor and modify it.  This can be done using `_`.  To do this, we first make a copy of a tensor using `tensor.clone()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = x.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8049, -0.0204]],\n",
       "\n",
       "         [[ 0.2558, -0.8377]],\n",
       "\n",
       "         [[-0.1351,  0.5858]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3349,  0.6151]],\n",
       "\n",
       "         [[ 1.2394,  1.4540]],\n",
       "\n",
       "         [[ 0.3045, -0.9893]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we do an in-place operation to add `x_copy` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.1516, -0.3126]],\n",
       "\n",
       "         [[-0.8538,  0.1927]],\n",
       "\n",
       "         [[ 0.3149,  0.9242]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3320,  2.0967]],\n",
       "\n",
       "         [[ 0.3307,  4.0127]],\n",
       "\n",
       "         [[-0.4137, -1.6834]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_copy.add_(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see that this tensor is modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.1516, -0.3126]],\n",
       "\n",
       "         [[-0.8538,  0.1927]],\n",
       "\n",
       "         [[ 0.3149,  0.9242]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3320,  2.0967]],\n",
       "\n",
       "         [[ 0.3307,  4.0127]],\n",
       "\n",
       "         [[-0.4137, -1.6834]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the original tensor is un-modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8049, -0.0204]],\n",
       "\n",
       "         [[ 0.2558, -0.8377]],\n",
       "\n",
       "         [[-0.1351,  0.5858]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3349,  0.6151]],\n",
       "\n",
       "         [[ 1.2394,  1.4540]],\n",
       "\n",
       "         [[ 0.3045, -0.9893]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other types of in-place operations:\n",
    "\n",
    "- `abs_()`\n",
    "- `acos_()`: inverse cosine of input and then save to the variable.\n",
    "- `addmm_(input-matrix, m1, m2)`: matrix multiply `m1` and `m2`, then add to `input-matrix`.\n",
    "\n",
    "... etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=mathematical-operations></a>\n",
    "### 2.3 Mathematical Operations\n",
    "\n",
    "#### Pointwise Ops\n",
    "\n",
    "- bitwise ops: `torch.bitwise_not`, `torch.bitwise_xor`\n",
    "- `torch.clamp(input, min, max)`: \"clamp\" all elements to range of \\[`min`, `max`\\].\n",
    "- `torch.ceil(input)`: returns the closest highest integer ceiling of all elements\n",
    "- `torch.floor(input)`: returns the closest lowest integer floor of all elements\n",
    "- `torch.div`: btw, if you want to do integer div, you can typecast the tensors to int\n",
    "- `torch.erf`, `torch.erfc`: error functions\n",
    "- `torch.exp`, `torch.log`, `torch.log10`, `torch.log2`: just log is natural log\n",
    "- `torch.mul(input, other, out = None)`: $out^*_i = other \\times input_i$\n",
    "- `torch.neg(input)`: change signs of input\n",
    "- `torch.pow()`: exponentials, element-wise\n",
    "- `torch.reciprocal()`: returns 1/input, element-wise\n",
    "- `torch.sin()`,`torch.tanh()`,`torch.sinh()`,`torch.cosh()`: trig functions, element-wise\n",
    "- `torch.sqrt()`\n",
    "- `torch.sigmoid()`\n",
    "\n",
    "\n",
    "#### Reduction Ops\n",
    "- `torch.argmin`,`torch.argmax`,`torch.min`,`torch.max` \n",
    "- `torch.mean`, `torch.median`, `torch.mode`\n",
    "- `torch.prod`: product of all elements in a tensor\n",
    "- `torch.sum`: sum of all elements in a tensor\n",
    "- `torch.unique`: tensor of all the unique elements in input tensor\n",
    "- `torch.argsort`, `torch.sort`\n",
    "\n",
    "\n",
    "#### Comparison Ops\n",
    "- `torch.eq`: element-wise equality\n",
    "- `torch.equal`: tensor-wise equality\n",
    "- `torch.le`, `torch.ge`, `torch.gt`: element-wise comparisons -> tensor of bools\n",
    "- `torch.topk(input,largest=True)`: topk elements\n",
    "\n",
    "\n",
    "#### Spectral Ops\n",
    "This part mostly contains Fourier transforms. Might be useful for Computer Vision, but will not go over this at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few of these functions.  I'm gonna start with **divisions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u:  tensor([6, 6, 5, 6, 0, 4, 2, 3, 4, 3])\n",
      "v:  tensor([5, 6, 3, 2, 3, 6, 7, 2, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 3, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.randint(0,10,(10,))\n",
    "v = torch.randint(0,10,(10,))\n",
    "print(\"u: \",u)\n",
    "print(\"v: \",v)\n",
    "\n",
    "u.div(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u:  tensor([6., 6., 5., 6., 0., 4., 2., 3., 4., 3.])\n",
      "v:  tensor([5., 6., 3., 2., 3., 6., 7., 2., 8., 8.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.2000, 1.0000, 1.6667, 3.0000, 0.0000, 0.6667, 0.2857, 1.5000, 0.5000,\n",
       "        0.3750])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = u.float()\n",
    "v = v.float()\n",
    "print(\"u: \",u)\n",
    "print(\"v: \",v)\n",
    "\n",
    "u.div(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're gonna try **sorting**.\n",
    "\n",
    "Sorting is operated along given dimensions.  If dimension not given, then it will pick the last dimension of input.\n",
    "First, we figure out how x looks like, and what the dimension of this vector is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions:  torch.Size([2, 3, 1, 2])\n",
      "print the tensor:\n",
      " tensor([[[[ 0.8049, -0.0204]],\n",
      "\n",
      "         [[ 0.2558, -0.8377]],\n",
      "\n",
      "         [[-0.1351,  0.5858]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3349,  0.6151]],\n",
      "\n",
      "         [[ 1.2394,  1.4540]],\n",
      "\n",
      "         [[ 0.3045, -0.9893]]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"dimensions: \",x.size())\n",
    "print(\"print the tensor:\\n\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default sort\n",
    "sorted_x, argsort_x = x.sort(descending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0204,  0.8049]],\n",
       "\n",
       "         [[-0.8377,  0.2558]],\n",
       "\n",
       "         [[-0.1351,  0.5858]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6151,  1.3349]],\n",
       "\n",
       "         [[ 1.2394,  1.4540]],\n",
       "\n",
       "         [[-0.9893,  0.3045]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort with dimensions\n",
    "sorted_x, argsort_x = x.sort(descending = False, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1351, -0.8377]],\n",
       "\n",
       "         [[ 0.2558, -0.0204]],\n",
       "\n",
       "         [[ 0.8049,  0.5858]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3045, -0.9893]],\n",
       "\n",
       "         [[ 1.2394,  0.6151]],\n",
       "\n",
       "         [[ 1.3349,  1.4540]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/sort-along-first.png\" width=\"280\" align=\"left\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2, 1]],\n",
       "\n",
       "         [[1, 0]],\n",
       "\n",
       "         [[0, 2]]],\n",
       "\n",
       "\n",
       "        [[[2, 2]],\n",
       "\n",
       "         [[1, 0]],\n",
       "\n",
       "         [[0, 1]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index is also along the axis\n",
    "argsort_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x, argsort_x = x.sort(descending = False, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8049, -0.0204]],\n",
       "\n",
       "         [[ 0.2558, -0.8377]],\n",
       "\n",
       "         [[-0.1351, -0.9893]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3349,  0.6151]],\n",
       "\n",
       "         [[ 1.2394,  1.4540]],\n",
       "\n",
       "         [[ 0.3045,  0.5858]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/sort-along-second.png\" width=\"350\" align=\"left\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0]],\n",
       "\n",
       "         [[0, 0]],\n",
       "\n",
       "         [[0, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1]],\n",
       "\n",
       "         [[1, 1]],\n",
       "\n",
       "         [[1, 0]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argsort_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=broadcasting></a>\n",
    "### 2.4 Broadcasting\n",
    "\n",
    "Going over pytorch's Broadcasting semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=matrix-multiplication></a>\n",
    "### 2.5 Matrix Multiplications\n",
    "\n",
    "- `torch.matmul`: matrix multiplication that **broadcasts**\n",
    "- `torch.mm`: matrix multiplication that **does not broadcast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
