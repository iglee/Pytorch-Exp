{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Different types of tensors using pytorch\n",
    "\n",
    "1. empty tensor - uninitialized.  the tensor dimensions can be specified, but it's empty.\n",
    "\n",
    "`torch.empty(DIMENSIONS)`\n",
    "2. randomly initialized tensor\n",
    "\n",
    "`torch.rand(DIMENSIONS)`\n",
    "3. tensor initialized with zeros\n",
    "\n",
    "`torch.zeros(DIMENSIONS)`\n",
    "4. tensor initialized with ones\n",
    "\n",
    "`torch.ones(DIMENSIONS)`\n",
    "5. randomly initialized with normal distribution random numbers $\\mathcal{N} (\\mu = 0, \\sigma^2 = 1)$\n",
    "\n",
    "`torch.randn_like(DIMENSIONS)` $\\rightarrow$ we can initialize using `_like()` functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try random initialization with normal distribution in 5.  Given a tensor, `randn_like` will initialize that function with normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_randn = torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure, we can try to plot the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x, sorted_indices = x_randn.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sorted_x, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cool.  I'm better convinced that this is normally random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Operations\n",
    "\n",
    "1. [Addition](#addition)\n",
    "2. [in-place operations](#in-place-operations)\n",
    "3. [mathematical operations](#mathematical-operations)\n",
    "4. [broadcasting](#broadcasting)\n",
    "5. [matrix multiplication](#matrix-multiplication)\n",
    "    - dot product\n",
    "    - matrix product\n",
    "6. changing dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='addition'></a>\n",
    "### 2.1 Addition \n",
    "\n",
    "First, we define a couple tensors for our purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0341,  0.2004]],\n",
       "\n",
       "         [[-0.6410, -1.2692]],\n",
       "\n",
       "         [[ 1.2683,  0.3775]]],\n",
       "\n",
       "\n",
       "        [[[-1.5204,  1.1605]],\n",
       "\n",
       "         [[ 0.4699, -1.1930]],\n",
       "\n",
       "         [[ 0.0153, -0.1653]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at this, let's break down the dimensions and count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([\n",
    "        [    \n",
    "             [   [ 1.1677,  0.2297]   ],\n",
    "\n",
    "             [   [-0.5983,  1.2568]   ],\n",
    "\n",
    "             [   [-1.2132,  0.0813]   ]\n",
    "        ],\n",
    "\n",
    "\n",
    "        [    \n",
    "             [   [-0.4010,  0.4659]   ],\n",
    "\n",
    "             [   [ 1.2019,  0.5684]   ],\n",
    "\n",
    "             [   [ 0.5998,  0.2074]   ]\n",
    "        ]\n",
    "       ])\n",
    "```\n",
    "Spacing out to make clear how dimensions are represented in pytorch.  Note that the $3^{rd}$ dimension is not represented, since it's 1.\n",
    "\n",
    "Now we define couple more vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(2,3,1,2)\n",
    "z = torch.randn(2,3,2  )\n",
    "a = torch.randn(  3,1,2)\n",
    "b = torch.randn(2,3,1  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use various representation to add two tensors: `+`, `torch.add(tensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9359, -0.5165]],\n",
       "\n",
       "         [[-0.6658, -1.5960]],\n",
       "\n",
       "         [[ 1.7028,  0.3911]]],\n",
       "\n",
       "\n",
       "        [[[-0.4211,  1.1572]],\n",
       "\n",
       "         [[ 1.4036, -1.3754]],\n",
       "\n",
       "         [[ 0.6938, -0.4454]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9359, -0.5165]],\n",
       "\n",
       "         [[-0.6658, -1.5960]],\n",
       "\n",
       "         [[ 1.7028,  0.3911]]],\n",
       "\n",
       "\n",
       "        [[[-0.4211,  1.1572]],\n",
       "\n",
       "         [[ 1.4036, -1.3754]],\n",
       "\n",
       "         [[ 0.6938, -0.4454]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9359, -0.5165]],\n",
       "\n",
       "         [[-0.6658, -1.5960]],\n",
       "\n",
       "         [[ 1.7028,  0.3911]]],\n",
       "\n",
       "\n",
       "        [[[-0.4211,  1.1572]],\n",
       "\n",
       "         [[ 1.4036, -1.3754]],\n",
       "\n",
       "         [[ 0.6938, -0.4454]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= in-place-operations></a>\n",
    "### 2.2 In-place operations\n",
    "\n",
    "Sometimes we want to operate on a tensor and modify it.  This can be done using `_`.  To do this, we first make a copy of a tensor using `tensor.clone()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = x.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0341,  0.2004]],\n",
       "\n",
       "         [[-0.6410, -1.2692]],\n",
       "\n",
       "         [[ 1.2683,  0.3775]]],\n",
       "\n",
       "\n",
       "        [[[-1.5204,  1.1605]],\n",
       "\n",
       "         [[ 0.4699, -1.1930]],\n",
       "\n",
       "         [[ 0.0153, -0.1653]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we do an in-place operation to add `x_copy` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9359, -0.5165]],\n",
       "\n",
       "         [[-0.6658, -1.5960]],\n",
       "\n",
       "         [[ 1.7028,  0.3911]]],\n",
       "\n",
       "\n",
       "        [[[-0.4211,  1.1572]],\n",
       "\n",
       "         [[ 1.4036, -1.3754]],\n",
       "\n",
       "         [[ 0.6938, -0.4454]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_copy.add_(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see that this tensor is modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9359, -0.5165]],\n",
       "\n",
       "         [[-0.6658, -1.5960]],\n",
       "\n",
       "         [[ 1.7028,  0.3911]]],\n",
       "\n",
       "\n",
       "        [[[-0.4211,  1.1572]],\n",
       "\n",
       "         [[ 1.4036, -1.3754]],\n",
       "\n",
       "         [[ 0.6938, -0.4454]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the original tensor is un-modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0341,  0.2004]],\n",
       "\n",
       "         [[-0.6410, -1.2692]],\n",
       "\n",
       "         [[ 1.2683,  0.3775]]],\n",
       "\n",
       "\n",
       "        [[[-1.5204,  1.1605]],\n",
       "\n",
       "         [[ 0.4699, -1.1930]],\n",
       "\n",
       "         [[ 0.0153, -0.1653]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other types of in-place operations:\n",
    "\n",
    "- `abs_()`\n",
    "- `acos_()`: inverse cosine of input and then save to the variable.\n",
    "- `addmm_(input-matrix, m1, m2)`: matrix multiply `m1` and `m2`, then add to `input-matrix`.\n",
    "\n",
    "... etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=mathematical-operations></a>\n",
    "### 2.3 Mathematical Operations\n",
    "\n",
    "#### Pointwise Ops\n",
    "\n",
    "- bitwise ops: `torch.bitwise_not`, `torch.bitwise_xor`\n",
    "- `torch.clamp(input, min, max)`: \"clamp\" all elements to range of \\[`min`, `max`\\].\n",
    "- `torch.ceil(input)`: returns the closest highest integer ceiling of all elements\n",
    "- `torch.floor(input)`: returns the closest lowest integer floor of all elements\n",
    "- `torch.div`: btw, if you want to do integer div, you can typecast the tensors to int\n",
    "- `torch.erf`, `torch.erfc`: error functions\n",
    "- `torch.exp`, `torch.log`, `torch.log10`, `torch.log2`: just log is natural log\n",
    "- `torch.mul(input, other, out = None)`: $out^*_i = other \\times input_i$\n",
    "- `torch.neg(input)`: change signs of input\n",
    "- `torch.pow()`: exponentials, element-wise\n",
    "- `torch.reciprocal()`: returns 1/input, element-wise\n",
    "- `torch.sin()`,`torch.tanh()`,`torch.sinh()`,`torch.cosh()`: trig functions, element-wise\n",
    "- `torch.sqrt()`\n",
    "- `torch.sigmoid()`\n",
    "\n",
    "\n",
    "#### Reduction Ops\n",
    "- `torch.argmin`,`torch.argmax`,`torch.min`,`torch.max` \n",
    "- `torch.mean`, `torch.median`, `torch.mode`\n",
    "- `torch.prod`: product of all elements in a tensor\n",
    "- `torch.sum`: sum of all elements in a tensor\n",
    "- `torch.unique`: tensor of all the unique elements in input tensor\n",
    "- `torch.argsort`, `torch.sort`\n",
    "\n",
    "\n",
    "#### Comparison Ops\n",
    "- `torch.eq`: element-wise equality\n",
    "- `torch.equal`: tensor-wise equality\n",
    "- `torch.le`, `torch.ge`, `torch.gt`: element-wise comparisons -> tensor of bools\n",
    "- `torch.topk(input,largest=True)`: topk elements\n",
    "\n",
    "\n",
    "#### Spectral Ops\n",
    "This part mostly contains Fourier transforms. Might be useful for Computer Vision, but will not go over this at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few of these functions.  I'm gonna start with **divisions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u:  tensor([2, 3, 7, 4, 1, 9, 4, 6, 0, 9])\n",
      "v:  tensor([1, 3, 3, 2, 5, 2, 4, 1, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 2, 0, 4, 1, 6, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.randint(0,10,(10,))\n",
    "v = torch.randint(0,10,(10,))\n",
    "print(\"u: \",u)\n",
    "print(\"v: \",v)\n",
    "\n",
    "u.div(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u:  tensor([2., 3., 7., 4., 1., 9., 4., 6., 0., 9.])\n",
      "v:  tensor([1., 3., 3., 2., 5., 2., 4., 1., 6., 6.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 1.0000, 2.3333, 2.0000, 0.2000, 4.5000, 1.0000, 6.0000, 0.0000,\n",
       "        1.5000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = u.float()\n",
    "v = v.float()\n",
    "print(\"u: \",u)\n",
    "print(\"v: \",v)\n",
    "\n",
    "u.div(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're gonna try **sorting**.\n",
    "\n",
    "Sorting is operated along given dimensions.  If dimension not given, then it will pick the last dimension of input.\n",
    "First, we figure out how x looks like, and what the dimension of this vector is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions:  torch.Size([2, 3, 1, 2])\n",
      "print the tensor:\n",
      " tensor([[[[ 1.0341,  0.2004]],\n",
      "\n",
      "         [[-0.6410, -1.2692]],\n",
      "\n",
      "         [[ 1.2683,  0.3775]]],\n",
      "\n",
      "\n",
      "        [[[-1.5204,  1.1605]],\n",
      "\n",
      "         [[ 0.4699, -1.1930]],\n",
      "\n",
      "         [[ 0.0153, -0.1653]]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"dimensions: \",x.size())\n",
    "print(\"print the tensor:\\n\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default sort\n",
    "sorted_x, argsort_x = x.sort(descending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2004,  1.0341]],\n",
       "\n",
       "         [[-1.2692, -0.6410]],\n",
       "\n",
       "         [[ 0.3775,  1.2683]]],\n",
       "\n",
       "\n",
       "        [[[-1.5204,  1.1605]],\n",
       "\n",
       "         [[-1.1930,  0.4699]],\n",
       "\n",
       "         [[-0.1653,  0.0153]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort with dimensions\n",
    "sorted_x, argsort_x = x.sort(descending = False, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6410, -1.2692]],\n",
       "\n",
       "         [[ 1.0341,  0.2004]],\n",
       "\n",
       "         [[ 1.2683,  0.3775]]],\n",
       "\n",
       "\n",
       "        [[[-1.5204, -1.1930]],\n",
       "\n",
       "         [[ 0.0153, -0.1653]],\n",
       "\n",
       "         [[ 0.4699,  1.1605]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/sort-along-first.png\" width=\"280\" align=\"left\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1]],\n",
       "\n",
       "         [[0, 0]],\n",
       "\n",
       "         [[2, 2]]],\n",
       "\n",
       "\n",
       "        [[[0, 1]],\n",
       "\n",
       "         [[2, 2]],\n",
       "\n",
       "         [[1, 0]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index is also along the axis\n",
    "argsort_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x, argsort_x = x.sort(descending = False, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.5204,  0.2004]],\n",
       "\n",
       "         [[-0.6410, -1.2692]],\n",
       "\n",
       "         [[ 0.0153, -0.1653]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0341,  1.1605]],\n",
       "\n",
       "         [[ 0.4699, -1.1930]],\n",
       "\n",
       "         [[ 1.2683,  0.3775]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/sort-along-second.png\" width=\"350\" align=\"left\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0]],\n",
       "\n",
       "         [[0, 0]],\n",
       "\n",
       "         [[1, 1]]],\n",
       "\n",
       "\n",
       "        [[[0, 1]],\n",
       "\n",
       "         [[1, 1]],\n",
       "\n",
       "         [[0, 0]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argsort_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=broadcasting></a>\n",
    "### 2.4 Broadcasting\n",
    "\n",
    "Going over pytorch's Broadcasting semantics.\n",
    "\n",
    "The semantics are actually quite similar to `numpy` Broadcasting semantics. \n",
    "\n",
    "#### 2.4.1 Review of `numpy` Broadcasting Semantics\n",
    "doc reference: https://numpy.org/doc/stable/user/basics.broadcasting.html#module-numpy.doc.broadcasting\n",
    "\n",
    "Broadcasting refers to how `numpy` deals with vectors of different sizes during arithmetic operations. Under certain constraints, smaller arrays are broadcast to larger array shape to make arithmetic operations compatible.  For instance, pointwise operations require that two arrays involved in operations have exactly the same shape. Broadcasting relaxes these constraints.\n",
    "\n",
    "\n",
    "**Broadcasting rules:**\n",
    "\n",
    "Two dimensions are considered compatible when\n",
    " - they're equal\n",
    " - one of them is 1\n",
    "\n",
    "Line up the vector dimensions from the right side, and compare the values according to above to determine whether or not they're compatible.  The trailing dimensions have to be compatible for the operation to broadcast.\n",
    "\n",
    "Resulting dimension would be the \n",
    "\n",
    "An example shown below (from reference `np` page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = x.reshape(4,1)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.ones(5)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.ones((3,4))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is x+y a possible operation? why or why not is it possible?  what do you think is the output value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-299960b562c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (5,) "
     ]
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about xx + y?  why or why not is it possible? what do you think is the output value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3.],\n",
       "       [4., 4., 4., 4., 4.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx+y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xx` has the trailing dimension of 1, and `y`'s last dimension is 5. Then, we can see that according to broadcasting semantics, the resulting output shape would be `(4,5)`. Now, when it comes to values, `xx` has to \"stretch\" their 2nd dimension. Then, `xx` becomes\n",
    "\n",
    "```\n",
    "array([[0,0,0,0,0],\n",
    "       [1,1,1,1,1],\n",
    "       [2,2,2,2,2],\n",
    "       [3,3,3,3,3]])\n",
    "```\n",
    "`y` will also \"stretch\" their 1st dimension to 4.  Which means `y` will become\n",
    "```\n",
    "array([1., 1., 1., 1., 1.],\n",
    "      [1., 1., 1., 1., 1.],\n",
    "      [1., 1., 1., 1., 1.],\n",
    "      [1., 1., 1., 1., 1.])\n",
    "```\n",
    "Then, the resulting value will become:\n",
    "```\n",
    "array([[1,1,1,1,1],\n",
    "       [2,2,2,2,2],\n",
    "       [3,3,3,3,3],\n",
    "       [4,4,4,4,4]])\n",
    "```\n",
    "Am I right or am I right? Welp, you expand the result above and find out :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=matrix-multiplication></a>\n",
    "### 2.5 Matrix Multiplications\n",
    "\n",
    "- `torch.matmul`: matrix multiplication that **broadcasts**\n",
    "- `torch.mm`: matrix multiplication that **does not broadcast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
